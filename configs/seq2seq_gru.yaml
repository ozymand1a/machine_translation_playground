model_name: seq2seq_gru

base_model:
  enc_emb_dim: 256
  enc_hid_dim: 512
  enc_dropout: 0.5
  dec_emb_dim: 256
  dec_hid_dim: 512
  dec_dropout: 0.5

# configure optimization
optimization:
  loss:
    name: 'bce'

  optimizer:
    path: torch.optim.AdamW
    params:
      lr: 0.0003
      betas: !!python/tuple [0.5, 0.999]
      weight_decay: 0.01

  scheduler:
    path: torch.optim.lr_scheduler.ReduceLROnPlateau
    monitor: val_loss
    step_size: 10
    gamma: 0.2
    interval: epoch
    frequency: 1
    strict: true
    params:
      factor: 0.5
      patience: 7
      min_lr: 1.0e-5
      verbose: true

logging:
  save_dir: logs

checkpoint_callback:
  dirpath: weights
  mode: min
  monitor: val_loss
  save_top_k: 1
  verbose: true

trainer:
  gpus: '1' #1'
  distributed_backend: null
  accumulate_grad_batches: 1
  profiler: false
  max_epochs: 5
  flush_logs_every_n_steps: 10
  #  log_save_interval: 1
  gradient_clip_val: 0.5
  num_sanity_val_steps: -1
  check_val_every_n_epoch: 1

seed: 1234

data:
  batch_size: 128
  epoch_size: 10

  with_pad: false
  inversion: false
  batch_first: false
  clip: 1
